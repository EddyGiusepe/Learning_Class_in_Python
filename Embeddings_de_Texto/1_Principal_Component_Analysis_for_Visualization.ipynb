{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"red\">Principal Component Analysis for Visualization</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Análise de Componentes Principais](https://machinelearningmastery.com/principal-component-analysis-for-visualization/) (PCA) é uma técnica de aprendizado de máquina `não supervisionada`. Talvez o uso mais popular da análise de componentes principais seja a `redução de dimensionalidade`. Além de usar a PCA como uma técnica de preparação de dados, também podemos usá-la para ajudar a visualizar dados. Uma imagem vale mais que mil palavras. Com os dados visualizados, é mais fácil para nós obter alguns insights e decidir sobre o próximo passo em nossos modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Gráfico de dispersão de dados de alta dimensão</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualização é um passo crucial para obter insights de dados. Podemos aprender com a visualização se um padrão pode ser observado e, portanto, estimar qual modelo de machine learning é adequado.\n",
    "\n",
    "É fácil representar coisas em duas dimensões. Normalmente, um gráfico de dispersão com eixos `x` e `y` são bidimensionais. Representar coisas em três dimensões é um pouco desafiador, mas não impossível. No `matplotlib`, por exemplo, é possível representar em 3D. O único problema é que no papel ou na tela, só podemos olhar para um gráfico 3D em uma janela de visualização ou projeção por vez. No `matplotlib`, isso é controlado pelo grau de elevação e azimute. Representar coisas em quatro ou cinco dimensões é impossível porque vivemos em um mundo tridimensional e não temos ideia de como as coisas em uma dimensão tão alta seriam.\n",
    "\n",
    "É aqui que uma `técnica de redução de dimensionalidade como PCA` entra em cena. Podemos reduzir a dimensão para duas ou três para que possamos visualizá-la. Vamos começar com um exemplo.\n",
    "\n",
    "Começamos com o [conjunto de dados do vinho](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset), que é um conjunto de dados de classificação com `13` features (ou seja, o conjunto de dados é `13` dimensional) e `3` classes. Há `178` amostras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
